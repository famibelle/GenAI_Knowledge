import torch
import pandas as pd
import os
from tqdm import tqdm
import argparse

# Solution pour PyTorch 2.6+ - Ajouter les classes TTS aux globals s√ªres
try:
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts, XttsArgs, XttsAudioConfig
    from TTS.config.shared_configs import BaseDatasetConfig, BaseAudioConfig
    
    # Ajouter les classes (pas les strings) aux globals s√ªres
    torch.serialization.add_safe_globals([
        XttsConfig,
        Xtts, 
        XttsArgs,
        XttsAudioConfig,
        BaseDatasetConfig,
        BaseAudioConfig
    ])
    print("üîê Classes TTS ajout√©es aux globals s√ªres")
    
except ImportError as e:
    print(f"‚ö†Ô∏è  Impossible d'importer certaines classes TTS: {e}")
    print("üìù Le chargement pourrait √©chouer, mais on continue...")

from TTS.api import TTS

# Patch pour corriger le probl√®me GPT2 avec transformers 4.50+
def apply_gpt2_patch():
    """
    Applique un patch global pour corriger le probl√®me GPT2InferenceModel
    """
    try:
        from transformers.models.gpt2.modeling_gpt2 import GPT2PreTrainedModel
        from transformers.generation.utils import GenerationMixin
        
        # V√©rifier si GPT2PreTrainedModel h√©rite d√©j√† de GenerationMixin
        if not issubclass(GPT2PreTrainedModel, GenerationMixin):
            print("üîß Application du patch GPT2 global...")
            
            # Cr√©er une nouvelle classe qui h√©rite des deux
            class PatchedGPT2PreTrainedModel(GPT2PreTrainedModel, GenerationMixin):
                pass
            
            # Remplacer la classe dans le module
            import transformers.models.gpt2.modeling_gpt2
            transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel = PatchedGPT2PreTrainedModel
            
            # Aussi patcher GPT2LMHeadModel si il existe
            if hasattr(transformers.models.gpt2.modeling_gpt2, 'GPT2LMHeadModel'):
                class PatchedGPT2LMHeadModel(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel, GenerationMixin):
                    pass
                transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel = PatchedGPT2LMHeadModel
            
            print("‚úÖ Patch GPT2 global appliqu√© avec succ√®s")
            return True
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Patch GPT2 global √©chou√©: {e}")
        return False
    
    return False

# Appliquer le patch avant de charger TTS
print("üîß Application du patch de compatibilit√© transformers...")
patch_success = apply_gpt2_patch()

# Patch alternatif pour forcer weights_only=False globalement
print("üîß Configuration de torch.load pour compatibilit√©...")
import torch
original_torch_load = torch.load

def patched_torch_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return original_torch_load(*args, **kwargs)

torch.load = patched_torch_load

# Configuration GPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîß Utilisation du device: {DEVICE}")


def split_text_by_sentences(text, max_chars=250):
    """
    Divise un texte en segments plus courts en respectant les phrases
    """
    import re
    
    # Nettoyer le texte
    text = str(text).strip()
    
    # Si le texte est d√©j√† court, le retourner tel quel
    if len(text) <= max_chars:
        return [text]
    
    # Diviser par phrases (points, points d'exclamation, points d'interrogation)
    sentences = re.split(r'[.!?]+', text)
    
    segments = []
    current_segment = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # Si ajouter cette phrase d√©passe la limite, sauvegarder le segment actuel
        if len(current_segment + sentence) > max_chars and current_segment:
            segments.append(current_segment.strip())
            current_segment = sentence
        else:
            if current_segment:
                current_segment += ". " + sentence
            else:
                current_segment = sentence
    
    # Ajouter le dernier segment
    if current_segment.strip():
        segments.append(current_segment.strip())
    
    # Si un segment est encore trop long, le diviser par virgules/points-virgules
    final_segments = []
    for segment in segments:
        if len(segment) <= max_chars:
            final_segments.append(segment)
        else:
            # Diviser par virgules
            parts = re.split(r'[,;]+', segment)
            temp_segment = ""
            for part in parts:
                part = part.strip()
                if len(temp_segment + part) > max_chars and temp_segment:
                    final_segments.append(temp_segment.strip())
                    temp_segment = part
                else:
                    if temp_segment:
                        temp_segment += ", " + part
                    else:
                        temp_segment = part
            if temp_segment.strip():
                final_segments.append(temp_segment.strip())
    
    return [seg for seg in final_segments if seg.strip()]


def generate_audio_with_fallback(tts_model, text, output_path, speaker_wav, language="fr"):
    """
    G√©n√®re de l'audio avec gestion des erreurs et division automatique des textes longs
    """
    import gc
    
    # V√©rifier la longueur du texte
    if len(text) > 273:
        print(f"üìù Texte trop long ({len(text)} caract√®res), division en segments...")
        
        segments = split_text_by_sentences(text, max_chars=250)
        print(f"üî¢ Division en {len(segments)} segments")
        
        # G√©n√©rer chaque segment
        segment_files = []
        for i, segment in enumerate(segments):
            segment_path = output_path.replace('.wav', f'_segment_{i+1}.wav')
            
            try:
                print(f"üéôÔ∏è  G√©n√©ration segment {i+1}/{len(segments)}: {segment[:50]}...")
                
                # Essayer plusieurs m√©thodes pour chaque segment
                success = False
                
                # M√©thode 1: Standard
                try:
                    tts_model.tts_to_file(
                        text=segment,
                        file_path=segment_path,
                        speaker_wav=speaker_wav,
                        language=language,
                        split_sentences=False
                    )
                    success = True
                except Exception as e1:
                    print(f"‚ö†Ô∏è  M√©thode 1 √©chou√©e: {e1}")
                
                # M√©thode 2: Recharger le mod√®le et appliquer le patch
                if not success:
                    try:
                        print("üîÑ Rechargement du mod√®le avec patch...")
                        del tts_model
                        gc.collect()
                        
                        # Recharger
                        tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False)
                        tts_model.to(DEVICE)
                        
                        # Appliquer le patch GPT2
                        patch_gpt2_model(tts_model)
                        
                        tts_model.tts_to_file(
                            text=segment,
                            file_path=segment_path,
                            speaker_wav=speaker_wav,
                            language=language,
                            split_sentences=False,
                            # Param√®tres pour une voix plus chaleureuse
                            temperature=0.75,  # Plus √©lev√© = plus d'expression (0.1-1.5)
                            length_penalty=1.0,  # Contr√¥le le rythme
                            repetition_penalty=5.0,  # √âvite les r√©p√©titions
                            top_k=50,  # Diversit√© du vocabulaire
                            top_p=0.85,  # Nucleus sampling pour plus de naturel
                        )
                        success = True
                    except Exception as e2:
                        print(f"‚ö†Ô∏è  M√©thode 2 √©chou√©e: {e2}")
                        
                        # M√©thode 3: Downgrade temporaire de transformers
                        try:
                            print("üîß Tentative de correction avec version compatible transformers...")
                            
                            # Forcer l'ajout de GenerationMixin
                            if hasattr(tts_model.synthesizer.tts_model, 'gpt'):
                                gpt_model = tts_model.synthesizer.tts_model.gpt
                                
                                # M√©thode alternative: cr√©er une m√©thode generate simple
                                def simple_generate(self, input_ids, **kwargs):
                                    # Utiliser forward ou __call__ comme fallback
                                    if hasattr(self, 'forward'):
                                        return self.forward(input_ids, **kwargs)
                                    else:
                                        return self(input_ids, **kwargs)
                                
                                # Attacher la m√©thode
                                import types
                                gpt_model.generate = types.MethodType(simple_generate, gpt_model)
                                
                                # Retry la g√©n√©ration
                                tts_model.tts_to_file(
                                    text=segment,
                                    file_path=segment_path,
                                    speaker_wav=speaker_wav,
                                    language=language,
                                    split_sentences=False
                                )
                                success = True
                                
                        except Exception as e3:
                            print(f"‚ö†Ô∏è  M√©thode 3 √©chou√©e: {e3}")
                
                if success:
                    segment_files.append(segment_path)
                    print(f"‚úÖ Segment {i+1} g√©n√©r√©")
                else:
                    print(f"‚ùå √âchec du segment {i+1}")
                    return False, tts_model
                    
            except Exception as segment_error:
                print(f"‚ùå Erreur segment {i+1}: {segment_error}")
                return False, tts_model
        
        # Combiner les segments avec pydub
        try:
            from pydub import AudioSegment
            
            print("üîó Combinaison des segments...")
            combined = AudioSegment.empty()
            
            for segment_file in segment_files:
                if os.path.exists(segment_file):
                    audio_segment = AudioSegment.from_wav(segment_file)
                    combined += audio_segment
                    combined += AudioSegment.silent(duration=300)  # 300ms de pause entre segments
                    
                    # Supprimer le fichier temporaire
                    os.remove(segment_file)
            
            # Sauvegarder le fichier final
            combined.export(output_path, format="wav")
            print("‚úÖ Segments combin√©s avec succ√®s")
            return True, tts_model
            
        except ImportError:
            print("‚ùå pydub non install√©. Installation: pip install pydub")
            print("üí° Les segments restent s√©par√©s dans le dossier")
            return True, tts_model
        except Exception as combine_error:
            print(f"‚ùå Erreur lors de la combinaison: {combine_error}")
            print("üí° Les segments restent s√©par√©s dans le dossier")
            return True, tts_model
    
    else:
        # Texte court, g√©n√©ration directe
        try:
            tts_model.tts_to_file(
                text=text,
                file_path=output_path,
                speaker_wav=speaker_wav,
                language=language,
                split_sentences=False,
                # Param√®tres pour une voix plus chaleureuse
                temperature=0.55,
                length_penalty=0.9,
                repetition_penalty=1.5,
                top_k=30,
                top_p=0.7,
            )
            return True, tts_model
            
        except AttributeError as attr_error:
            if "'GPT2InferenceModel' object has no attribute 'generate'" in str(attr_error):
                print("üîÑ Correction GPT2 pour texte court...")
                try:
                    # Appliquer le patch GPT2
                    patch_gpt2_model(tts_model)
                    
                    # Retry
                    tts_model.tts_to_file(
                        text=text,
                        file_path=output_path,
                        speaker_wav=speaker_wav,
                        language=language,
                        split_sentences=False
                    )
                    return True, tts_model
                    
                except Exception as patch_error:
                    print(f"‚ùå Patch √©chou√©: {patch_error}")
                    
                    # M√©thode de fallback ultime: recharger compl√®tement
                    try:
                        del tts_model
                        gc.collect()
                        
                        # R√©initialiser avec une approche diff√©rente
                        print("üîÑ R√©initialisation compl√®te du mod√®le...")
                        
                        # Forcer weights_only=False pour √©viter d'autres probl√®mes
                        original_load = torch.load
                        torch.load = lambda *args, **kwargs: original_load(*args, **{**kwargs, 'weights_only': False})
                        
                        tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=False)
                        tts_model.to(DEVICE)
                        
                        # Restaurer torch.load
                        torch.load = original_load
                        
                        # Appliquer le patch imm√©diatement
                        patch_gpt2_model(tts_model)
                        
                        # Retry final
                        tts_model.tts_to_file(
                            text=text,
                            file_path=output_path,
                            speaker_wav=speaker_wav,
                            language=language,
                            split_sentences=False
                        )
                        return True, tts_model
                        
                    except Exception as final_reload_error:
                        print(f"‚ùå √âchec final apr√®s rechargement: {final_reload_error}")
                        return False, tts_model
            else:
                raise attr_error
        except Exception as other_error:
            print(f"‚ùå Autre erreur: {other_error}")
            return False, tts_model


# Charger le mod√®le XTTS-v2
print("üì• Chargement du mod√®le XTTS-v2...")

def patch_gpt2_model(tts_model):
    """
    Patch le mod√®le GPT2 pour corriger le probl√®me de generate
    """
    try:
        if hasattr(tts_model.synthesizer.tts_model, 'gpt'):
            gpt_model = tts_model.synthesizer.tts_model.gpt
            
            if not hasattr(gpt_model, 'generate'):
                print("üîß Correction du mod√®le GPT2 individuel...")
                
                # M√©thode 1: Ajouter GenerationMixin √† la classe
                from transformers.generation.utils import GenerationMixin
                
                # Cr√©er une nouvelle classe qui h√©rite de GenerationMixin
                original_class = gpt_model.__class__
                
                class PatchedGPTModel(original_class, GenerationMixin):
                    def __init__(self, *args, **kwargs):
                        super().__init__(*args, **kwargs)
                
                # Remplacer la classe de l'instance
                gpt_model.__class__ = PatchedGPTModel
                
                # Forcer l'initialisation des attributs GenerationMixin si n√©cessaire
                if not hasattr(gpt_model, 'generation_config'):
                    from transformers import GenerationConfig
                    gpt_model.generation_config = GenerationConfig()
                
                # V√©rifier que generate fonctionne maintenant
                if hasattr(gpt_model, 'generate'):
                    print("‚úÖ Mod√®le GPT2 corrig√© avec succ√®s")
                    return True
                else:
                    # M√©thode alternative: cr√©er generate manuellement
                    def custom_generate(self, input_ids, **kwargs):
                        # Version simplifi√©e de generate
                        max_length = kwargs.get('max_length', 50)
                        current_length = input_ids.shape[1]
                        
                        for _ in range(max_length - current_length):
                            outputs = self(input_ids)
                            next_token_logits = outputs.logits[:, -1, :]
                            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                            input_ids = torch.cat([input_ids, next_token], dim=1)
                            
                            # Simple stopping criterion
                            if next_token.item() in [50256, 0]:  # EOS tokens
                                break
                                
                        return type('GenerateOutput', (), {'sequences': input_ids})()
                    
                    import types
                    gpt_model.generate = types.MethodType(custom_generate, gpt_model)
                    print("‚úÖ M√©thode generate personnalis√©e ajout√©e")
                    return True
                    
            else:
                print("‚úÖ Mod√®le GPT2 d√©j√† fonctionnel")
                return True
                
    except Exception as patch_error:
        print(f"‚ùå √âchec du patch GPT2: {patch_error}")
        
        # M√©thode de dernier recours
        try:
            if hasattr(tts_model.synthesizer.tts_model, 'gpt'):
                gpt_model = tts_model.synthesizer.tts_model.gpt
                
                # Cr√©er une m√©thode generate tr√®s simple
                def emergency_generate(self, input_ids, **kwargs):
                    # Juste faire un forward pass
                    outputs = self(input_ids)
                    return type('Output', (), {'sequences': input_ids, 'logits': outputs.logits})()
                
                import types
                gpt_model.generate = types.MethodType(emergency_generate, gpt_model)
                print("üö® M√©thode generate d'urgence appliqu√©e")
                return True
        except:
            pass
        
        return False
    
    return False

try:
    tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=True)
    tts.to(DEVICE)
    print("‚úÖ Mod√®le charg√© avec succ√®s")
    
    # Appliquer le patch pour corriger le probl√®me GPT2
    patch_success = patch_gpt2_model(tts)
    if not patch_success:
        print("‚ö†Ô∏è  Le patch GPT2 a √©chou√©, le mod√®le pourrait avoir des probl√®mes")
    
except Exception as e:
    print(f"‚ùå Erreur lors du chargement: {e}")
    print("üí° Tentative avec weights_only=False...")
    
    try:
        # Sauvegarder la fonction originale
        original_load = torch.load
        
        # Cr√©er une fonction qui force weights_only=False
        def patched_load(*args, **kwargs):
            kwargs['weights_only'] = False
            return original_load(*args, **kwargs)
        
        # Appliquer le patch temporairement
        torch.load = patched_load
        
        # Charger le mod√®le
        tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=True)
        tts.to(DEVICE)
        
        # Restaurer la fonction originale
        torch.load = original_load
        
        print("‚úÖ Mod√®le charg√© avec succ√®s (weights_only=False)")
        
    except Exception as final_error:
        print(f"‚ùå √âchec final: {final_error}")
        print("üí° Solutions possibles:")
        print("   - pip install --upgrade TTS transformers torch")
        print("   - pip uninstall TTS && pip install TTS==0.22.0")
        print("   - Red√©marrer le kernel/environnement Python")
        exit(1)

# --- Ajout du support CLI ---
def parse_args():
    parser = argparse.ArgumentParser(description="G√©n√©ration audio XTTS batch")
    parser.add_argument('--excel', type=str, default="VoixOff/Voix Off.xlsx", help="Fichier Excel des voix off")
    parser.add_argument('--ref', type=str, default="Voices/M√©dhiCloneHigh.wav", help="Fichier wav de r√©f√©rence pour le clonage de voix")
    parser.add_argument('--out', type=str, default="Generated/coqui-xtts_v2", help="Dossier de sortie")
    return parser.parse_args()

args = parse_args()
EXCEL_PATH = args.excel
REFERENCE_WAV = args.ref
OUTPUT_DIR = args.out

# Param√®tres
# EXCEL_PATH = "VoixOff/Voix Off.xlsx"
# REFERENCE_WAV = "Voices/M√©dhiCloneHigh.wav"
# OUTPUT_DIR = "Generated/coqui-xtts_v2"

# Cr√©er le dossier de sortie s'il n'existe pas
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"üìÅ Dossier de sortie: {OUTPUT_DIR}")

# V√©rifier que les fichiers existent
if not os.path.exists(EXCEL_PATH):
    print(f"‚ùå Fichier Excel non trouv√©: {EXCEL_PATH}")
    exit(1)

if not os.path.exists(REFERENCE_WAV):
    print(f"‚ùå Fichier de r√©f√©rence vocal non trouv√©: {REFERENCE_WAV}")
    exit(1)

try:
    # Charger le fichier Excel
    print("üìä Chargement du fichier Excel...")
    data = pd.read_excel(EXCEL_PATH)
    print(f"‚úÖ {len(data)} lignes trouv√©es dans le fichier Excel")
    
    # Afficher les colonnes disponibles pour debug
    print(f"üìã Colonnes disponibles: {list(data.columns)}")
    
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du fichier Excel: {e}")
    exit(1)

# Parcourir chaque ligne du fichier Excel
successful_generations = 0
failed_generations = 0

for index, row in tqdm(data.iterrows(), total=len(data), desc="G√©n√©ration audio"):
    try:
        slide_number = row['Slide Number']
        slide_title = str(row['Slide Title']).replace('/', '_').replace('\\', '_')  # Nettoyer le titre pour le nom de fichier
        voice_text = row['Voice Over Text']
        
        # V√©rifier que le texte n'est pas vide
        if pd.isna(voice_text) or str(voice_text).strip() == '':
            print(f"‚ö†Ô∏è  Slide {slide_number}: Texte vide, passage √† la suivante")
            continue
        
        # G√©n√©rer le nom du fichier de sortie
        output_path = f"{OUTPUT_DIR}/{os.path.basename(EXCEL_PATH).replace('.xlsx', '')}_Slide{slide_number}_{slide_title}_{os.path.basename(EXCEL_PATH)}.wav"
        
        # Utiliser la fonction de g√©n√©ration robuste
        print(f"üéôÔ∏è  Traitement Slide {slide_number}: {slide_title[:50]}...")
        print(f"üìù Longueur du texte: {len(str(voice_text))} caract√®res")
        
        success, tts = generate_audio_with_fallback(
            tts, 
            str(voice_text), 
            output_path, 
            REFERENCE_WAV, 
            "fr"
        )
        
        if success:
            print(f"‚úÖ Fichier g√©n√©r√© : {output_path}")
            successful_generations += 1
        else:
            print(f"‚ùå √âchec de la g√©n√©ration pour Slide {slide_number}")
            failed_generations += 1
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration pour la ligne {index + 1}: {e}")
        failed_generations += 1
        continue

print(f"\nüéâ G√©n√©ration termin√©e!")
print(f"‚úÖ Succ√®s: {successful_generations}")
print(f"‚ùå √âchecs: {failed_generations}")
print(f"üìÅ Fichiers g√©n√©r√©s dans: {OUTPUT_DIR}")